# ELEN644_Project: New Deep Multi-Modal Network for Book Genre Classification Based on Cover Images using Transformers


## Problem Statment: 
Most real-world interactions are usually multi-modal or multi-sensory, but historically, machine learning models are usually single-modal. Previous work in multi-modal book genre classification by book cover have shown promising results. However, with a 56.1\% top-1 accuracy for 30 classes, even the state-of-the-art models of yesteryear have plenty of room for improvement. Previous work in this area have employed limited methods and have not explored solutions using newer deep learning models. Limited work has also been done in determining the effects of applying pre-processing techniques for genre classification. For these reasons, this paper can serve as an additional data point to the community with respect to the larger question of deep learning architectures beyond CNNs for classification and the role of transformers in machine learning in general.

We have organized our folders based on the approaches we were applying for genre classification image-based, text-based and multimodal. Instructions to run the code for each approach is contained with each folder. 




